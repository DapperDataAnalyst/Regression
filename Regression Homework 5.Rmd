---
title: "Regression Homework 5"
output: html_document
date: "2023-02-24"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Problem 1

#### Part (1)

We are given that $\nu = P\epsilon$. We will use this to find $E(\nu_i)$:
$$E(\nu_i) = E(P_i\epsilon_i)$$
$$= E(P_i) * E(\epsilon_i)$$
$$= E(P_i) * 0$$
$$= 0$$
**Thus, $E(\nu_i) = 0$**

To find the variance we will use the standard formula for variance, which is $E(X^2) - (E(X))^2$. We also note that the matrix P is orthonormal, meaning that $P'P = I$. Putting this all together, we can find $Var(\nu_i)$ as follows:
$$Var(\nu_i) = E(\nu_i^2) - E(\nu_i)^2$$
$$= E(\nu' \nu) - 0$$
$$= E(\epsilon' P' P \epsilon)$$
$$= E(\epsilon' I \epsilon) = 1$$
**Thus we have shown that $Var(\nu_i) = 1$**

---

#### Part (2)

We are given that $\gamma = Q\beta$ and $PXQ' = D$. Combining these two equations to form an expression for $D\gamma$ yields $D\gamma = PXQ'Q\beta$. We know that that Q is orthonormal, meaning that $Q'Q = I$. Using this fact, we find that $D\gamma = PXQ'Q\beta = PX\beta$.

**Thus we have shown that $PX\beta$ can be written as $D\gamma$ and thus $z_i = \begin{cases} \lambda_{i}\gamma_{i} + \sigma v_{i} &\text{i=1,...,p} \\ \sigma v_{i} &\text{i = p + 1,...,n,}\end{cases}$**

---

#### Part (3)

Under the original model, before SVD, the covariance matrix is found as $\sigma^2(X'X)^{-1}$. Because the variances of the betas lie along the diagonal of the covariance matrix, $\sum Var(\hat{\beta}_j) = trace(\sigma^2(X'X)^{-1})$. After SVD, $D$ is the analog to $X$, so the covariance matrix after SVD is $\sigma^2(D'D)^{-1} = \sigma^2(\Lambda^2)$. The $jth$ diagonal element of $\Lambda^2$ is $\lambda^2_j$, thus the $jth$ diagonal element of $(\Lambda^2)^{-1}$ is $\frac{1}{\lambda^2_j}$. 

**Putting this all together, $\sum Var(\hat{\beta}_j) = trace(\sigma^2(X'X)^{-1}) = trace(\sigma^2(\Lambda^2)^{-1}) = \sigma^2 \sum \frac{1}{\lambda^2_j}$**

---

#### Part (4)

```
data = data.frame(read.csv("~/Downloads/HW5_data.csv", header = TRUE))
x = as.matrix(data.frame(data[,-1]))
y = matrix(data[,1], nrow = n, ncol = 1)
n = 200
p = 5
P = t(svd(x, nu = n, nv = p)$u)
Q = t(svd(x, nu = n, nv = p)$v)
big_lambda = diag(svd(x, nu = n, nv = p)$d) # Contains the eigenvalues, not squared
D = P%*%x%*%t(Q)
z = P%*%y


# Problem 1.4
gamma_hat = 0
for (i in 1:p){
  gamma_hat[i] = z[i]/big_lambda[i,i]
}

vect_o_eigens = 0
for (i in 1:p) {
  vect_o_eigens[i] = big_lambda[i,i]
}

```
**Solution:**
\[
  \hat{\gamma} =
  \left[ {\begin{array}{cc}
    0.88428498 \\
    0.06425593 \\
    1.66167113 \\
    -1.41376844 \\
    0.30486612
  \end{array} } \right]
\]

---

#### Part (5)

```
# Problem 1.5
ss_eigens = sum(vect_o_eigens^2)

quotient1 = vect_o_eigens[1]^2/ss_eigens
# = 0.5664857

quotient2 = sum(vect_o_eigens[1:2]^2)/ss_eigens
# = 0.7648945

quotient3 = sum(vect_o_eigens[1:3]^2)/ss_eigens
# = 0.8804599

quotient4 = sum(vect_o_eigens[1:4]^2)/ss_eigens
# = 0.9746963

quotient5 = sum(vect_o_eigens[1:5]^2)/ss_eigens
# = 1.00

# So then...
m = 4

bh_pca = t(Q[1:m,1:m])%*%gamma_hat[1:m]
```
A value of $m = 4$ is the first value that brings the quotient above 0.9, so $m = 4$.

```
m = 4

bh_pca = t(Q[1:m,1:m])%*%gamma_hat[1:m]
```

\[
  \hat{\beta_{pca}} =
  \left[ {\begin{array}{cc}
    1.57021141 \\
    -0.96725821 \\
    0.11719105 \\
    0.05748972
  \end{array} } \right]
\]

## Problem 2

#### Part (1)

The likelihood function can be found as the following:
$$I(\beta) = exp[-\frac{\sum_{i=1}^n(y_i-x_i\beta)^2}{2\sigma^2}]$$
$$= exp[-\frac{\sum_{i=1}^ny_i^2-2\sum_{i=1}^nx_iy_i\beta + \sum_{i=1}^nx_i^2\beta^2}{2\sigma^2}]$$

We are given that $\sum_{i=1}^n x_i = 0$, $\sum_{i=1}^nx_i^2 = n$, and $\sum_{i=1}^n x_iy_i=\gamma$. Substituting these into the likelihood function above yields:
$$I(\beta) = exp[-\frac{\sum_{i=1}^ny_i^2-2\gamma\beta+n\beta^2}{2\sigma^2}]$$

The prior function is found to be as follows:
$$\pi(\beta) = 0.5 \times1(\beta=0)+0.5N(\beta|0,\tau^2)$$

The posterior density is found to be as follows:
$$\pi(\beta|y_{1:m}) \propto exp\{\frac{\sum_{i=1}^ny_i^2-2\gamma\beta+n\beta^2}{2\sigma^2} \}\space \{0.5 \times1(\beta=0)+0.5N(\beta|0,\tau^2\}$$
$$0.5N(\beta|0, \tau^2) = 0.5\space exp\{-\frac{\beta^2}{2\tau^2}\}$$
$$\pi(\beta|y_{1:m}) \propto \text{exp}\{\frac{\sum_{i=1}^ny_i^2-2\gamma\beta+n\beta^2}{2\sigma^2} \}\space \{0.5 \times1(\beta=0)+0.5\space \text{exp}\{-\frac{\beta^2}{2\tau^2}\}\} \text{  or  } \text{exp}\{\frac{\sum_{i=1}^n(y_i^2-x_i\beta)^2}{2\sigma^2} \}\space \{0.5 \times1(\beta=0)+0.5\space \text{exp}\{-\frac{\beta^2}{2\tau^2}\}$$

---

#### Part (2)

The marginal likelihood can be found as follows:

$$m(y_1,...,y_n) = \int\prod_{i=1}^n N(y_i\space | \space x_i\beta, \sigma^2) \space N(\beta\space |\space 0, \tau^2)\space d\beta$$
$$m(y_1,...,y_n) = \int(\frac{1}{\sigma\sqrt{2\pi}})^n\space \text{exp}(\frac{\sum_{i=1}^n(y_i-x_i\beta)^2}{2\sigma^2})\space \frac{1}{\tau\sqrt{2\pi}}
\text{exp} (-\frac{\beta^2}{2\tau^2})d\beta$$


$$m(y_1,...,y_n)= \int (\frac{1}{\sigma\sqrt{2\pi}})^n\space \text{exp}(-\frac{\sum_{i=1}^ny_i^2-2\gamma\beta+n\beta^2}{2\sigma^2})\space \frac{1}{\tau\sqrt{2\pi}}\text{exp}(-\frac{\beta^2}{2\tau^2})d\beta$$
$$m(y_1,...,y_n) = (\frac{1}{\sigma\sqrt{2\pi}})^n\space \frac{1}{\tau\sqrt{2\pi}} \text{exp}(-\frac{\sum_{i=1}^ny^2}{2\sigma^2})\int \text{exp}(\frac{2\gamma\beta}{2\sigma^2}-\frac{n\beta^2}{2\sigma^2}-\frac{\beta^2}{2\tau^2})\space d\beta$$

We now focus on the integral from the previous equation:

$$\int\text{exp}(-\frac{1}{2}\frac{-2\beta\gamma}{\sigma^2}+\frac{n\beta^2}{\sigma^2}+\frac{\beta^2}{\tau^2})d\beta = \int\text{exp}( -\frac{1}{2}\frac{-2\beta\gamma\tau^2\space +\space n\beta^2\tau^2\space +\space\beta^2\sigma^2}{\sigma^2\tau^2}\space)d\beta$$
$$= \int \text{exp}(-\frac{1}{2}\frac{(n\tau^2+\sigma^2)\beta^2}{\sigma^2\tau^2}-\frac{2(\sigma\tau^2)\beta}{\sigma^2\tau^2})d\beta$$
$$= \int \text{exp}(-\frac{1}{2}\frac{n\tau^2+\sigma^2}{\sigma^2\tau^2}(\beta^2-2(\frac{\gamma\tau^2}{n\tau^2+\sigma^2})\beta))d\beta$$
$$= \int \text{exp}(-\frac{1}{2}\frac{n\tau^2+\sigma^2}{\sigma^2\tau^2}(\beta^2-2(\frac{\gamma\tau^2}{n\tau^2+\sigma^2})\beta + (\frac{\sigma\tau^2}{n\tau^2+\sigma^2})^2-(\frac{\sigma\tau^2}{n\tau^2+\sigma^2})^2))d\beta$$
$$= \int\text{exp}(-\frac{1}{2}\frac{n\tau^2+\sigma^2}{\sigma^2\tau^2}((\beta-\frac{\gamma\tau^2}{n\tau^2+\gamma^2})^2-(\frac{\gamma\tau^2}{n\tau^2+\sigma^2})^2)d\beta \text{, (by completing the square)}$$

We can now remove the term $(\frac{\gamma\tau^2}{n\tau^2+\sigma^2})^2$ in order to put the equation into the form of a normal density. Note that the integral over any probability density function is 1. The equation can now be written as the following:

$$= \int \text{exp}(-\frac{1}{2(1/(n/\sigma^2+1/\tau^2))}(\beta-\frac{\gamma}{n+\sigma^2/\tau^2}))d\beta = 1$$

**Performing the integration removes all $\beta$ terms, leaving us with the following, which is the final answer:**

$$m(y_1,...,y_n) = (\frac{1}{\sigma\sqrt{2\pi}})^n\space \frac{1}{\tau\sqrt{2\pi}} \text{exp}(-\frac{\sum_{i=1}^ny^2}{2\sigma^2})\cdot \text{exp}(-\frac{1}{2}\frac{\gamma^2}{\sigma^2}\frac{\tau^2}{n\tau^2+\sigma^2})$$

---

#### Part (3)

We can show that,

$$\pi(\beta\space |\space y_1,...,y_n) \propto \{\prod_{i=1}^n N(y_i|0,\sigma^2) \}\space 1(\beta=0)\space +\space m(y_1,...,y_m)N(\beta|\nu,\psi^2)$$

and find $\nu$ and $\psi$ as follows:

$$\pi(\beta\space |\space y_1,...,y_n) \propto (\frac{1}{\sigma \sqrt{2\pi}})^n \text{exp}(-\frac{1}{2}\frac{\sum_{i=1}^ny_i^2}{\sigma^2})\space\cdot\space 1(\beta=0)\space +\space (\frac{1}{\sigma\sqrt{2\pi}})^n \space(\frac{1}{\tau\sqrt{2\pi}})\space \text{exp}(-\frac{1}{2}\frac{\sum_{i=1}^ny_i^2}{\sigma^2})\space \text{exp}(-\frac{1}{2}\frac{\gamma^2\tau^2}{\sigma^2(n\tau^2+\sigma^2)})\space\frac{1}{\sqrt{2\pi\psi^2}}\text{exp}(-\frac{1}{2}\frac{(\beta^2-2\nu\beta)}{\psi^2})$$

$$\psi^2 = \frac{1}{n/\sigma^2 + 1/\tau^2}$$

$$\nu = \frac{\gamma}{n+\sigma^2/\tau^2}$$

---

#### Part (4)

We can write down $\pi(\beta=0|y_1,...y_n)$ as follows:

$$\pi(\beta=0\space |\space y_1,...,y_n) \propto (\frac{1}{\sigma \sqrt{2\pi}})^n \text{exp}(-\frac{1}{2}\frac{\sum_{i=1}^ny_i^2}{\sigma^2})\space+\space (\frac{1}{\sigma\sqrt{2\pi}})^n \space(\frac{1}{\tau\sqrt{2\pi}})\space \text{exp}(-\frac{1}{2}\frac{\sum_{i=1}^ny_i^2}{\sigma^2})\space \text{exp}(\frac{\gamma^2\tau^2}{2\sigma^2(n\tau^2+\sigma^2)})\space\frac{1}{\sqrt{2\pi\psi^2}}\text{exp}(-\frac{1}{2}\frac{\nu^2}{\psi^2})$$

---

#### Part (5)

Note that

$$\frac{m(y_1,...,y_n)}{\prod_{i=1}^nN(y_i|0,\sigma^2)} = \frac{(\frac{1}{\sigma\sqrt{2\pi}})^n \space(\frac{1}{\tau\sqrt{2\pi}})\space \text{exp}(\frac{-\sum_{i=1}^ny_i^2}{2\sigma^2})\space \text{exp}(\frac{\gamma^2\tau^2}{2\sigma^2(n\tau^2+\sigma^2)})}{(\frac{1}{\sigma \sqrt{2\pi}})^n\text{exp}(\frac{-\sum_{i=1}^ny_i^2}{2\sigma^2})}$$

We are given that $\sum_{i=1}^nx_iy_i = \gamma$. From this equation, we can see that, as the $y^2$ terms in the above equation increase, so too will the $\gamma^2$ terms.

We note that the numerator of the expression above, $m(y_1,...,y_n)$, is the likelihood function of the regression model with a normal prior, and the denominator, $\prod_{i=1}^nN(y_i|0,\sigma^2)$, is the likelihood function of the regression model with all beta coefficients taking values of zero.

If $\sum_{i=1}^nx_iy_i = \gamma$ is large, the numerator dominates. If $\sum_{i=1}^nx_iy_i = \gamma$ is small, the denominator dominates. **Thus, the ratio expressed above is a Bayes factor, with a large value supporting the alternative hypothesis, and a small value supporting the null hypothesis.**



